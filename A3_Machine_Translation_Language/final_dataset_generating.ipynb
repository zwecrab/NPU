{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample data written to my_01.txt.\n",
      "Dataset successfully written to dataset/translation/dataset.csv\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# Regular expression to filter out marker lines (e.g. \"#4/5\", \"104\", etc.)\n",
    "MARKER_PATTERN = re.compile(r'^\\s*#?\\d+(\\/\\d+)?\\s*$')\n",
    "\n",
    "def filter_lines(raw_lines):\n",
    "    \"\"\"\n",
    "    Remove blank lines and marker lines from the raw lines.\n",
    "    Returns a list of stripped, filtered lines.\n",
    "    \"\"\"\n",
    "    filtered = []\n",
    "    for line in raw_lines:\n",
    "        stripped = line.strip()\n",
    "        if not stripped:\n",
    "            continue  # skip blank lines\n",
    "        if MARKER_PATTERN.match(stripped):\n",
    "            continue  # skip marker lines\n",
    "        filtered.append(stripped)\n",
    "    return filtered\n",
    "\n",
    "def contains_myanmar(text):\n",
    "    \"\"\"\n",
    "    Returns True if the text contains at least one character in the Myanmar Unicode block.\n",
    "    The Myanmar block is roughly U+1000 to U+109F.\n",
    "    \"\"\"\n",
    "    for ch in text:\n",
    "        if '\\u1000' <= ch <= '\\u109F':\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "def is_myanmar(text):\n",
    "    \"\"\"\n",
    "    Returns True if the text is considered Myanmar text.\n",
    "    For our purposes, if it contains any Myanmar characters, we assume it is Myanmar.\n",
    "    \"\"\"\n",
    "    return contains_myanmar(text)\n",
    "\n",
    "def is_english(text):\n",
    "    \"\"\"\n",
    "    Returns True if the text does not contain any Myanmar characters.\n",
    "    \"\"\"\n",
    "    return not contains_myanmar(text)\n",
    "\n",
    "def pair_lines(filtered_lines):\n",
    "    \"\"\"\n",
    "    Iterate through the filtered lines in order and pair an English sentence\n",
    "    with the next Myanmar sentence that follows.\n",
    "    \n",
    "    If two (or more) English lines appear consecutively, the last one is used.\n",
    "    If a Myanmar line appears without a preceding English line, it is skipped.\n",
    "    \n",
    "    Returns a list of [English, Myanmar] pairs.\n",
    "    \"\"\"\n",
    "    pairs = []\n",
    "    pending_english = None\n",
    "    for line in filtered_lines:\n",
    "        if is_english(line):\n",
    "            # Update the pending English sentence.\n",
    "            pending_english = line\n",
    "        elif is_myanmar(line):\n",
    "            if pending_english:\n",
    "                pairs.append([pending_english, line])\n",
    "                pending_english = None  # clear after pairing\n",
    "            else:\n",
    "                # No pending English sentence; skip this Myanmar line.\n",
    "                continue\n",
    "    return pairs\n",
    "\n",
    "def clean_english(text):\n",
    "    \"\"\"\n",
    "    Remove all punctuation from the English text while preserving spaces.\n",
    "    This uses a regex that removes any character that is not a word character or whitespace.\n",
    "    \"\"\"\n",
    "    cleaned = re.sub(r'[^\\w\\s]', '', text)\n",
    "    return cleaned\n",
    "\n",
    "def clean_myanmar(text):\n",
    "    \"\"\"\n",
    "    Remove all punctuation and spaces from the Myanmar text.\n",
    "    This regex keeps only characters in the Myanmar Unicode range (U+1000 to U+109F).\n",
    "    \"\"\"\n",
    "    # Remove any character that is not in the Myanmar block.\n",
    "    cleaned = re.sub(r'[^\\u1000-\\u109F]', '', text)\n",
    "    return cleaned\n",
    "\n",
    "def write_sample_data():\n",
    "    \"\"\"\n",
    "    Writes sample data to 'my_01.txt' for demonstration purposes.\n",
    "    This sample intentionally includes extra blank lines and marker lines.\n",
    "    \n",
    "    Expected structure in the valid (filtered) lines:\n",
    "      - Some lines are English.\n",
    "      - Some lines are Myanmar.\n",
    "    \"\"\"\n",
    "    sample_data = (\n",
    "        \"Metadata header\\n\"             # (to be ignored)\n",
    "        \"\\n\"                            # blank line\n",
    "        \"Everybody gets champagne and some people don't drink it so what's left the kids drink so we were going around drinking all this champagne.\\n\"  # English (valid)\n",
    "        \"လူတိုင်းက ရှန်ပိန်တွေရပြီး တချို့လူတွေက မသောက်ကြဘူးဆိုတော့ ကျန်တာတွေကို ကလေးတွေက သောက်ကြတာဆိုတော့ ဒီရှန်ပိန်တွေကို ငါတို့ အကုန်သောက်ခဲ့တယ်။\\n\"  # Myanmar (valid)\n",
    "        \"Extra info\\n\"                  # ignored\n",
    "        \"More info\\n\"                   # ignored\n",
    "        \"I mean that was the whole point.\\n\"  # English (valid)\n",
    "        \"ငါဆိုလိုတာက ဒါက အဓိက ဘဲလေ။\\n\"  # Myanmar (valid)\n",
    "        \"#4/5\\n\"                        # marker (to be removed)\n",
    "        \"သူက ဘယ်အချိန်လဲဆိုတာ မပြောခဲ့ပြန်ဘူး၊ ဒီတော့ ငါ့လဲ အဲဒီမှာ စိတ်ပူနေရပြီး ဒါက ဘယ်အချိန်မှာ လိုမှာလဲဆိုတာ မသိတော့ဘူး။\\n\"  # English? (but contains Myanmar punctuation) – language detection will consider it Myanmar since it has Myanmar characters.\n",
    "        \"It was probably the first thing I remember from being a little kid about, ah, especially about something that I'd done wrong.\\n\"  # English (valid)\n",
    "        \"He is from Greece and he is from a small village in Greece called Tokalleka and he came to America and I believe it was 1969 or 1970 and he shortly got married.\\n\"  # English (valid)\n",
    "        \"သူကတော့ ဂရိ က လာတာဖြစ်ပြီး ဂရိ က Tokalleka ဆိုတဲ့ ရွာငယ်လေးကပါ ဒီနောက် သူအမေရိကားကို လာခဲ့တာ အဲဒါ ၁၉၆၉ သို့မဟုတ် ၁၉၇၀ ကလို့ ငါထင်တယ် ဒီနောက် မကြာခင်မှာပဲ သူအိပ်ထောင်ပြုခဲ့တယ်။\\n\"  # Myanmar (valid)\n",
    "        \"#5/5\\n\"                        # marker (remove)\n",
    "        \"Nobody knew where they went.\\n\"  # English (valid)\n",
    "        \"And they couldn't stay in the Augusta area because people knew that they had tried to do something that was really taboo and try to pass for white.\\n\"  # Myanmar? (language detection: English only characters so will be considered English)\n",
    "        \"We were watching something on TV.\\n\"  # English (valid)\n",
    "        \"ကျွန်တော်တို့ တီဗီမှာ တခုခု ကြည့်နေခဲ့သည်။\\n\"  # Myanmar (valid)\n",
    "        \"ဒီလို အသေးအမွှားလေးတွေက ငါလုပ်နေတာတွေပေါ် အကြီးအကျယ် ပြောင်းလဲစေခဲ့တယ်။\\n\"  # English? Actually, contains Myanmar characters so considered Myanmar.\n",
    "        \"\\\"Um, and she said, she said, she said, Baby, she said, You don't understand about life the way I understand about life.\\\"\\n\"  # English (valid)\n",
    "        \"သူပြောတယ်၊ သူပြောတယ်၊ သူပြောတယ်၊ ကလေးရယ် တဲ့၊  သူပြောတယ်၊ ဘဝအ‌‌‌ကြောင်းကို ငါနားလည်သလို မင်းနားမလည်ဘူး။\\n\"  # Myanmar (valid)\n",
    "        \"စီအိုင်အေက ဒီရုပ်ရှင်ကို ဖြုတ်ချပြီး နောက်တနေ့မှာ ကုလသမဂ္ဂဆီကို ယူသွားလိုက်တယ်။\\n\"  # English? Contains Myanmar letters so treated as Myanmar.\n",
    "        \"\\\"So I went to, I went to Washington D.C. and I didn't go directly to, uh, that, uh, they had told me to on my orders.\\\"\\n\"  # English (valid)\n",
    "        \"သူကတော့ အသားနဲနဲဖြူတဲ့ လူမဲတစ်ယောက်ပါ။\\n\"  # Myanmar (valid)\n",
    "        \"\\\"So anyway, Dad goes and makes this nice big glass of chocolate milk for me.\\\"\\n\"  # English (valid)\n",
    "        \"\\\"Um, and so they just left town, and she, she never did see her sister again, never saw her sister again.\\\"\\n\"  # English (valid)\n",
    "        \"ဒီတော့ သူတို့ မြို့ကနေ ပြောင်းခဲ့ကြပြီး၊ သူ သူ့ရဲ့ ညီမကို ဘယ်တော့မှ ပြန်မတွေ့ခဲ့ဘူ။ သူ့ ညီမကို ဘယ်တော့မှ ပြန်မတွေ့ခဲ့ဘူး။\\n\"  # Myanmar (valid)\n",
    "        \"\\\"OK, can you hear me?\\\"\\n\"  # English (valid)\n",
    "    )\n",
    "    \n",
    "    with open(\"my_01.txt\", 'w', encoding='utf-8') as f:\n",
    "        f.write(sample_data)\n",
    "    print(\"Sample data written to my_01.txt.\")\n",
    "\n",
    "def pair_and_clean(filtered_lines):\n",
    "    \"\"\"\n",
    "    Pair English and Myanmar sentences based on language detection,\n",
    "    then clean the sentences:\n",
    "      - Remove punctuation from both languages.\n",
    "      - Additionally remove spaces from Myanmar sentences.\n",
    "    \n",
    "    Returns a list of [clean_english, clean_myanmar] pairs.\n",
    "    \"\"\"\n",
    "    pairs = pair_lines(filtered_lines)\n",
    "    cleaned_pairs = []\n",
    "    for eng, myan in pairs:\n",
    "        clean_eng = clean_english(eng)\n",
    "        clean_myan = clean_myanmar(myan)\n",
    "        cleaned_pairs.append([clean_eng, clean_myan])\n",
    "    return cleaned_pairs\n",
    "\n",
    "def main():\n",
    "    all_data = []  # List to hold all [English, Myanmar] pairs\n",
    "    df = []\n",
    "    # Process files my_01.txt to my_100.txt.\n",
    "    for i in range(1, 101):\n",
    "        file_name = f\"dataset/translation/my_{i:02d}.txt\"  # e.g. my_01.txt, my_02.txt, etc.\n",
    "        try:\n",
    "            with open(file_name, 'r', encoding='utf-8') as f:\n",
    "                raw_lines = f.readlines()\n",
    "                # Filter out blank lines and marker lines.\n",
    "                filtered = filter_lines(raw_lines)\n",
    "                # Pair and clean the lines.\n",
    "                cleaned_pairs = pair_and_clean(filtered)\n",
    "                all_data.extend(cleaned_pairs)\n",
    "        except FileNotFoundError:\n",
    "            print(f\"File {file_name} not found. Skipping.\")\n",
    "            continue\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {file_name}: {e}\")\n",
    "            continue\n",
    "\n",
    "    # Write the collected, cleaned pairs to a CSV file.\n",
    "    output_file = 'dataset/translation/dataset.csv'\n",
    "    try:\n",
    "        with open(output_file, 'w', newline='', encoding='utf-8') as csvfile:\n",
    "            writer = csv.writer(csvfile)\n",
    "            writer.writerows(all_data)\n",
    "            df = pd.DataFrame(all_data, columns=['en', 'my'])\n",
    "        print(f\"Dataset successfully written to {output_file}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error writing dataset: {e}\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # For demonstration, write sample data to my_01.txt.\n",
    "    write_sample_data()\n",
    "    df = main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>en</th>\n",
       "      <th>my</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Everybody gets champagne and some people dont ...</td>\n",
       "      <td>လူတိုင်းကရှန်ပိန်တွေရပြီးတချို့လူတွေကမသောက်ကြဘ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I mean that was the whole point</td>\n",
       "      <td>ငါဆိုလိုတာကဒါကအဓိကဘဲလေ။</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>He is from Greece and he is from a small villa...</td>\n",
       "      <td>သူကတော့ဂရိကလာတာဖြစ်ပြီးဂရိကဆိုတဲ့ရွာငယ်လေးကပါဒ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>We were watching something on TV</td>\n",
       "      <td>ကျွန်တော်တို့တီဗီမှာတခုခုကြည့်နေခဲ့သည်။</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Um and she said she said she said Baby she sai...</td>\n",
       "      <td>သူပြောတယ်၊သူပြောတယ်၊သူပြောတယ်၊ကလေးရယ်တဲ့၊သူပြေ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9907</th>\n",
       "      <td>Davidson shouldnt talk in a way where bone and...</td>\n",
       "      <td>သည်နှင့်ကိုအသံထွက်ညီသည့်နည်းလမ်းတစ်ခုဖြင့်စကား...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9908</th>\n",
       "      <td>It would be better if Davidson rhymed the word...</td>\n",
       "      <td>သည်နှင့်ဟူသောစကားလုံးများကိုကာရံမိလျှင်ပိုကောင...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9909</th>\n",
       "      <td>A 200000 word novel at 25 is a fair price</td>\n",
       "      <td>စကားလုံး၂၀၀၀၀၀ရှိသောဝတ္ထုတစ်ပုဒ်ကို၂၅ဒေါ်လာဆို...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9910</th>\n",
       "      <td>A 200000 word novel for 25 is 4000 words per d...</td>\n",
       "      <td>စကားလုံး၂၀၀၀၀၀ရှိသည့်ဝတ္ထုတစ်ပုဒ်သည်၂၅ဒေါ်လာရှ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9911</th>\n",
       "      <td>A 200000 word novel for 25 is 8000 words per d...</td>\n",
       "      <td>စကားလုံး၂၀၀၀၀၀ရှိသည့်ဝတ္ထုတစ်ပုဒ်သည်၂၅ဒေါ်လာရှ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9912 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     en  \\\n",
       "0     Everybody gets champagne and some people dont ...   \n",
       "1                       I mean that was the whole point   \n",
       "2     He is from Greece and he is from a small villa...   \n",
       "3                      We were watching something on TV   \n",
       "4     Um and she said she said she said Baby she sai...   \n",
       "...                                                 ...   \n",
       "9907  Davidson shouldnt talk in a way where bone and...   \n",
       "9908  It would be better if Davidson rhymed the word...   \n",
       "9909          A 200000 word novel at 25 is a fair price   \n",
       "9910  A 200000 word novel for 25 is 4000 words per d...   \n",
       "9911  A 200000 word novel for 25 is 8000 words per d...   \n",
       "\n",
       "                                                     my  \n",
       "0     လူတိုင်းကရှန်ပိန်တွေရပြီးတချို့လူတွေကမသောက်ကြဘ...  \n",
       "1                               ငါဆိုလိုတာကဒါကအဓိကဘဲလေ။  \n",
       "2     သူကတော့ဂရိကလာတာဖြစ်ပြီးဂရိကဆိုတဲ့ရွာငယ်လေးကပါဒ...  \n",
       "3               ကျွန်တော်တို့တီဗီမှာတခုခုကြည့်နေခဲ့သည်။  \n",
       "4     သူပြောတယ်၊သူပြောတယ်၊သူပြောတယ်၊ကလေးရယ်တဲ့၊သူပြေ...  \n",
       "...                                                 ...  \n",
       "9907  သည်နှင့်ကိုအသံထွက်ညီသည့်နည်းလမ်းတစ်ခုဖြင့်စကား...  \n",
       "9908  သည်နှင့်ဟူသောစကားလုံးများကိုကာရံမိလျှင်ပိုကောင...  \n",
       "9909  စကားလုံး၂၀၀၀၀၀ရှိသောဝတ္ထုတစ်ပုဒ်ကို၂၅ဒေါ်လာဆို...  \n",
       "9910  စကားလုံး၂၀၀၀၀၀ရှိသည့်ဝတ္ထုတစ်ပုဒ်သည်၂၅ဒေါ်လာရှ...  \n",
       "9911  စကားလုံး၂၀၀၀၀၀ရှိသည့်ဝတ္ထုတစ်ပုဒ်သည်၂၅ဒေါ်လာရှ...  \n",
       "\n",
       "[9912 rows x 2 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from datasets import Dataset, DatasetDict\n",
    "\n",
    "# Split the dataframe into train, test, and validation sets\n",
    "train_df, temp_df = train_test_split(df, test_size=0.02, random_state=42)\n",
    "val_df, test_df = train_test_split(temp_df, test_size=0.5, random_state=42)\n",
    "\n",
    "# Reset the index to avoid '__index_level_0__' column\n",
    "train_df = train_df.reset_index(drop=True)\n",
    "val_df = val_df.reset_index(drop=True)\n",
    "test_df = test_df.reset_index(drop=True)\n",
    "\n",
    "# Convert the dataframes to Hugging Face datasets\n",
    "train_dataset = Dataset.from_pandas(train_df)\n",
    "val_dataset = Dataset.from_pandas(val_df)\n",
    "test_dataset = Dataset.from_pandas(test_df)\n",
    "\n",
    "# Create a DatasetDict\n",
    "dataset_dict = DatasetDict({\n",
    "    'train': train_dataset,\n",
    "    'validation': val_dataset,\n",
    "    'test': test_dataset\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving the dataset (1/1 shards): 100%|██████████| 9713/9713 [00:00<00:00, 1489553.01 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 99/99 [00:00<00:00, 41494.56 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 100/100 [00:00<00:00, 25003.30 examples/s]\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import HfApi, create_repo, login\n",
    "\n",
    "# the huggingface write token has been removed for security restrictions\n",
    "# Save the dataset to disk\n",
    "dataset_dict.save_to_disk('dataset/npu_a3_en_my')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating parquet from Arrow format: 100%|██████████| 10/10 [00:00<00:00, 794.04ba/s]\n",
      "Uploading the dataset shards: 100%|██████████| 1/1 [00:02<00:00,  2.41s/it]\n",
      "Creating parquet from Arrow format: 100%|██████████| 1/1 [00:00<00:00, 733.40ba/s]\n",
      "Uploading the dataset shards: 100%|██████████| 1/1 [00:01<00:00,  1.88s/it]\n",
      "Creating parquet from Arrow format: 100%|██████████| 1/1 [00:00<00:00, 677.59ba/s]\n",
      "Uploading the dataset shards: 100%|██████████| 1/1 [00:01<00:00,  1.16s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/datasets/st125338/npu_a3_en_my/commit/913dfc44311d4c4c322de86b820cd81f5613fa9b', commit_message='Upload dataset', commit_description='', oid='913dfc44311d4c4c322de86b820cd81f5613fa9b', pr_url=None, repo_url=RepoUrl('https://huggingface.co/datasets/st125338/npu_a3_en_my', endpoint='https://huggingface.co', repo_type='dataset', repo_id='st125338/npu_a3_en_my'), pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create the repository if it does not exist\n",
    "repo_id = 'st125338/npu_a3_en_my'\n",
    "create_repo(repo_id, repo_type='dataset', private=False)\n",
    "\n",
    "# Push the dataset to Hugging Face\n",
    "dataset_dict.push_to_hub(repo_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
